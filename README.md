## Simple Web Scraper & Preprocessing Pipeline

Lightweight tools to scrape one or more web pages and transform the results into clean, structured datasets you can use for downstream analysis.

- `simple_web_scraper.py`: Scrapes pages using Selenium + BeautifulSoup and saves both JSON and human-readable TXT.
- `simple_web_scraper_headless.py`: Same scraper, runs Chrome in headless mode.
- `preprocess_scraped_data.py`: Parses the scraper TXT output, cleans/deduplicates text, and emits enriched, processed JSON/TXT.

### Features
- **Any URL input**: single URL, multiple URLs, or a file of URLs (`urls.txt`).
- **Rich extraction**: title, headings, paragraphs, links, images, meta tags, an abstract (best-effort), and full text.
- **Friendly outputs**:
  - Raw: `data/{domain}_scraped_data.json` and `.txt`
  - Processed: `processed_data/{domain}_scraped_data_processed.json` and `.txt`
- **Stats included**: counts for headings, paragraphs, links, images and per-run processing metrics in the processed files.
- **Headless mode** for servers/CI.

### Requirements
- Python 3.10+
- Google Chrome installed (the driver is auto-managed by `webdriver-manager`)
- OS: Windows, macOS, or Linux

Install Python dependencies:

```bash
pip install -r requirements.txt
```

On Windows PowerShell, you may need:

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### Quick start
Scrape a single URL:

```bash
python simple_web_scraper.py https://example.com
```

Scrape multiple URLs:

```bash
python simple_web_scraper.py https://example.com https://news.ycombinator.com
```

Scrape from a file (one URL per line):

```bash
# edit urls.txt then run
python simple_web_scraper.py urls.txt
```

Run in headless mode (no visible browser window):

```bash
python simple_web_scraper_headless.py https://example.com
```

Interactive mode (no arguments):

```bash
python simple_web_scraper.py
```

### Output
When scraping `https://example.com`, two files are created in `data/`:

- `data/example_com_scraped_data.json`
- `data/example_com_scraped_data.txt`

The TXT is structured for human reading and for the preprocessor to parse. Example (truncated):

```text
WEBSITE SCRAPED DATA
==================================================

URL: https://example.com
Title: Example Domain

ABSTRACT:
...best-effort abstract if found...

HEADINGS:
H1: Example Domain

PARAGRAPHS:
1. This domain is for use in illustrative examples ...

LINKS (first 20):
1. More information... -> https://www.iana.org/domains/example

META TAGS:
description: ...

FULL TEXT LENGTH: 12345 characters
TOTAL HEADINGS: 3
TOTAL PARAGRAPHS: 20
TOTAL LINKS: 15
TOTAL IMAGES: 2
```

Note: The TXT file limits printed paragraphs/links to the first 20 for readability. The JSON contains the full arrays.

### Preprocessing the scraped data
Run the preprocessor to clean, normalize, and summarize the scraped TXT files:

```bash
python preprocess_scraped_data.py --input data --output processed_data
```

What it does:
- Parses the scraper TXT format (URL, Title, ABSTRACT, HEADINGS, PARAGRAPHS, LINKS, META TAGS, and stats).
- Cleans and normalizes text, removes duplicates and very short paragraphs.
- Preserves/cleans headings and link text, and copies meta tags and original stats.
- Adds processed metrics (e.g., counts, average paragraph length, total words, timestamp).
- Writes both JSON and nicely formatted TXT into `processed_data/` and logs to `preprocessing.log`.

Example outputs:
- `processed_data/example_com_scraped_data_processed.json`
- `processed_data/example_com_scraped_data_processed.txt`

### Project structure
```text
.
├─ data/                      # Raw scraper outputs (JSON + TXT)
├─ processed_data/            # Cleaned/processed outputs (JSON + TXT)
├─ preprocess_scraped_data.py # Preprocessing pipeline
├─ simple_web_scraper.py      # Scraper (interactive/CLI)
├─ simple_web_scraper_headless.py # Scraper (headless)
├─ urls.txt                   # Optional list of URLs to scrape
├─ requirements.txt
├─ preprocessing.log          # Generated by the preprocessor
└─ README.md
```

### Tips and troubleshooting
- **Chrome/Driver issues**: Ensure Chrome is installed and fairly up-to-date. `webdriver-manager` will auto-download a compatible driver.
- **Dynamic pages**: The scraper waits a fixed `time.sleep(5)`. Increase if needed for heavy pages.
- **Blocked by sites/CAPTCHA**: Consider slower scraping, rotating networks/proxies, or adding smarter waits; some sites may block automation.
- **Headless differences**: Some sites render differently headless. If content is missing, try the non-headless script.
- **Input format**: The preprocessor expects TXT files produced by these scraper scripts. Using other formats may fail to parse.




 

